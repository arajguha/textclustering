{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Clustering Using K-means and DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EHMuQtQNX3Ki"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import random\n",
    "import math\n",
    "from collections import defaultdict\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.utils import shuffle\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import calinski_harabaz_score\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading the CSR Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xBYVxUOEaHbB"
   },
   "outputs": [],
   "source": [
    "def csr_read(fname, ftype=\"csr\", nidx=1):\n",
    "    r\"\"\" \n",
    "        Read CSR matrix from a text file. \n",
    "        \n",
    "        \\param fname File name for CSR/CLU matrix\n",
    "        \\param ftype Input format. Acceptable formats are:\n",
    "            - csr - Compressed sparse row\n",
    "            - clu - Cluto format, i.e., CSR + header row with \"nrows ncols nnz\"\n",
    "        \\param nidx Indexing type in CSR file. What does numbering of feature IDs start with?\n",
    "    \"\"\"\n",
    "    \n",
    "    with open(fname) as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    if ftype == \"clu\":\n",
    "        p = lines[0].split()\n",
    "        nrows = int(p[0])\n",
    "        ncols = int(p[1])\n",
    "        nnz = long(p[2])\n",
    "        lines = lines[1:]\n",
    "        assert(len(lines) == nrows)\n",
    "    elif ftype == \"csr\":\n",
    "        nrows = len(lines)\n",
    "        ncols = 0 \n",
    "        nnz = 0 \n",
    "        for i in range(nrows):\n",
    "            p = lines[i].split()\n",
    "            if len(p) % 2 != 0:\n",
    "                raise ValueError(\"Invalid CSR matrix. Row %d contains %d numbers.\" % (i, len(p)))\n",
    "            nnz += len(p)/2\n",
    "            for j in range(0, len(p), 2): \n",
    "                cid = int(p[j]) - nidx\n",
    "                if cid+1 > ncols:\n",
    "                    ncols = cid+1\n",
    "    else:\n",
    "        raise ValueError(\"Invalid sparse matrix ftype '%s'.\" % ftype)\n",
    "    val = np.zeros(int(nnz), dtype=np.float)\n",
    "    ind = np.zeros(int(nnz), dtype=np.int)\n",
    "    ptr = np.zeros(nrows+1, dtype=np.long)\n",
    "    n = 0 \n",
    "    for i in range(nrows):\n",
    "        p = lines[i].split()\n",
    "        for j in range(0, len(p), 2): \n",
    "            ind[n] = int(p[j]) - nidx\n",
    "            val[n] = float(p[j+1])\n",
    "            n += 1\n",
    "        ptr[i+1] = n \n",
    "    \n",
    "    assert(n == nnz)\n",
    "    \n",
    "    return csr_matrix((val, ind, ptr), shape=(nrows, ncols), dtype=np.float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculating the IDF of the the input CSR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def csr_idf(matrix, copy=False, **kargs):\n",
    "    r\"\"\" Scale a CSR matrix by idf. \n",
    "    Returns scaling factors as dict. If copy is True, \n",
    "    returns scaled matrix and scaling factors.\n",
    "    \"\"\"\n",
    "    if copy is True:\n",
    "        matrix = matrix.copy()\n",
    "    nrows = matrix.shape[0]\n",
    "    nnz = matrix.nnz\n",
    "    ind, val, ptr = matrix.indices, matrix.data, matrix.indptr\n",
    "    # document frequency\n",
    "    df = defaultdict(int)\n",
    "    for i in ind:\n",
    "        df[i] += 1\n",
    "    # inverse document frequency\n",
    "    for k,v in df.items():\n",
    "        df[k] = np.log(nrows / float(v))  ## df turns to idf - reusing memory\n",
    "    # scale by idf\n",
    "    for i in range(0, nnz):\n",
    "        val[i] *= df[ind[i]]\n",
    "        \n",
    "    return df if copy is False else matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalising the Calculated IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def csr_l2normalize(matrix, copy=False, **kargs):\n",
    "    r\"\"\" Normalize the rows of a CSR matrix by their L-2 norm. \n",
    "    If copy is True, returns a copy of the normalized matrix.\n",
    "    \"\"\"\n",
    "    if copy is True:\n",
    "        matrix = matrix.copy()\n",
    "    nrows = matrix.shape[0]\n",
    "    nnz = matrix.nnz\n",
    "    ind, val, ptr = matrix.indices, matrix.data, matrix.indptr\n",
    "    # normalize\n",
    "    for i in range(nrows):\n",
    "        rsum = 0.0    \n",
    "        for j in range(ptr[i], ptr[i+1]):\n",
    "            rsum += val[j]**2\n",
    "        if rsum == 0.0:\n",
    "            continue  # do not normalize empty rows\n",
    "        rsum = float(1.0/np.sqrt(rsum))\n",
    "        for j in range(ptr[i], ptr[i+1]):\n",
    "            val[j] *= rsum\n",
    "            \n",
    "    if copy is True:\n",
    "        return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read CSR matrix from the input file\n",
    "csrMatrix = csr_read('train.dat', ftype=\"csr\", nidx=1)\n",
    "\n",
    "#Scale the CSR matrix by idf (Inverse Document Frequency)\n",
    "csrIDF = csr_idf(csrMatrix, copy=True)\n",
    "\n",
    "#Normalize the rows of a CSR matrix by their L-2 norm.\n",
    "csrL2Normalized = csr_l2normalize(csrIDF, copy=True)\n",
    "\n",
    "#Obtain a dense ndarray representation of the CSR matrix.\n",
    "denseMatrix = csrL2Normalized.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 13272)\t0.06993666116639771\n",
      "  (0, 556)\t0.031789842428550756\n",
      "  (0, 477)\t0.05053442765379448\n",
      "  (0, 956)\t0.02892827625486347\n",
      "  (0, 1229)\t0.028083587824155785\n",
      "  (0, 52)\t0.03222801243394182\n",
      "  (0, 54)\t0.036401876151556926\n",
      "  (0, 90)\t0.09576566392091451\n",
      "  (0, 1432)\t0.03333780832574419\n",
      "  (0, 3170)\t0.05174109794628905\n",
      "  (0, 56)\t0.030100101096961172\n",
      "  (0, 93)\t0.030590918020923865\n",
      "  (0, 94)\t0.030240896982303315\n",
      "  (0, 2525)\t0.06240393636859998\n",
      "  (0, 2842)\t0.03931861920976099\n",
      "  (0, 98)\t0.043712293625395346\n",
      "  (0, 4706)\t0.05465090467007069\n",
      "  (0, 2727)\t0.05318919552206405\n",
      "  (0, 120)\t0.013815811952246083\n",
      "  (0, 7093)\t0.09656247105989821\n",
      "  (0, 5077)\t0.07014198864903001\n",
      "  (0, 401)\t0.05200307147257695\n",
      "  (0, 4584)\t0.0604208761422968\n",
      "  (0, 520)\t0.09921245927919291\n",
      "  (0, 521)\t0.2878035569692616\n",
      "  :\t:\n",
      "  (8579, 978)\t0.04197962738062193\n",
      "  (8579, 1374)\t0.029514731682777906\n",
      "  (8579, 339)\t0.010532709229759157\n",
      "  (8579, 8564)\t0.05900450229942135\n",
      "  (8579, 5974)\t0.05868349749062223\n",
      "  (8579, 261)\t0.007703464115733322\n",
      "  (8579, 267)\t0.04189644766193121\n",
      "  (8579, 666)\t0.07713495798796251\n",
      "  (8579, 40)\t0.03456527276018497\n",
      "  (8579, 668)\t0.03459825810076994\n",
      "  (8579, 1303)\t0.0733118251044857\n",
      "  (8579, 2035)\t0.0748877071639371\n",
      "  (8579, 1380)\t0.04411269224851701\n",
      "  (8579, 5259)\t0.05733288934648154\n",
      "  (8579, 2273)\t0.13639706501660628\n",
      "  (8579, 5343)\t0.0746872175317015\n",
      "  (8579, 3880)\t0.047055442987402885\n",
      "  (8579, 1865)\t0.07177792820071746\n",
      "  (8579, 1946)\t0.03741484197777794\n",
      "  (8579, 112)\t0.028380594302867868\n",
      "  (8579, 1786)\t0.046092006070387916\n",
      "  (8579, 1868)\t0.03997856116126159\n",
      "  (8579, 9862)\t0.07719931395499881\n",
      "  (8579, 909)\t0.02717338002381583\n",
      "  (8579, 17069)\t0.08455752226320176\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(8580, 126355)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(csrL2Normalized)\n",
    "csrL2Normalized.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8580, 126355)\n"
     ]
    }
   ],
   "source": [
    "print(denseMatrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using the Sklearn's MiniBatch KMeans to find the initial clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MiniBatchKMeans(batch_size=100, compute_labels=True, init='k-means++',\n",
       "        init_size=None, max_iter=100, max_no_improvement=10,\n",
       "        n_clusters=200, n_init=3, random_state=0, reassignment_ratio=0.01,\n",
       "        tol=0.0, verbose=0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmeans = MiniBatchKMeans(n_clusters=200,random_state = 0)\n",
    "kmeans.fit(csrL2Normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = kmeans.labels_\n",
    "points =centroids= centers = kmeans.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "centers.shape, label.shape\n",
    "indices = np.asarray(list(range(0,8580)))\n",
    "lab = np.column_stack([indices,label])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### My Implementation of DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MyDBSCAN(points, eps,minPts):\n",
    "    neighborhoods = []\n",
    "    core = []\n",
    "    border = []\n",
    "    noise = []\n",
    "\n",
    "    for i in range(len(points)):\n",
    "        neighbors = []\n",
    "        for p in range(0, len(points)):\n",
    "            # If the distance is below eps, p is a neighbor\n",
    "            if sp.spatial.distance.cosine(points[i] ,points[p]) <= eps:\n",
    "                neighbors.append(p)\n",
    "        neighborhoods.append(neighbors)\n",
    "        # If neighborhood has at least minPts, i is a core point\n",
    "        if len(neighbors) >= minPts :\n",
    "            core.append(i)\n",
    "    # Find border points \n",
    "    for i in range(len(points)):\n",
    "        neighbors = neighborhoods[i]\n",
    "        # Look at points that are not core points\n",
    "        if len(neighbors) < minPts:\n",
    "            for j in range(len(neighbors)):\n",
    "                # If one of its neighbors is a core, it is also in the core point's neighborhood, \n",
    "                # thus it is a border point rather than a noise point\n",
    "                if neighbors[j] in core:\n",
    "                    border.append(i)\n",
    "                    # Need at least one core point...\n",
    "                    break\n",
    "    # Find noise points\n",
    "    for i in range(len(points)):\n",
    "        if i not in core and i not in border:\n",
    "            noise.append(i)\n",
    "            \n",
    "    # # Invoke graph instance to visualize the cluster\n",
    "    G = nx.Graph()\n",
    "    nodes = core\n",
    "    G.add_nodes_from(nodes)\n",
    "    # Create neighborhood\n",
    "    for i in range(len(nodes)):\n",
    "        for p in range(len(nodes)):\n",
    "            # If the distance is below the threshold, add a link in the graph.\n",
    "            if p != i and sp.spatial.distance.cosine(points[nodes[i]] ,points[nodes[p]]) <= eps:\n",
    "                G.add_edges_from([(nodes[i], nodes[p])])\n",
    "    # List the connected components / clusters\n",
    "    clusters = list(nx.connected_components(G))\n",
    "    print(\"# clusters:\", len(clusters))\n",
    "    print(\"clusters: \", clusters)\n",
    "    centers = []\n",
    "    for cluster in clusters:\n",
    "        coords = []\n",
    "        for point in list(cluster):\n",
    "            coords.append(points[point])\n",
    "        center = np.mean(coords,axis =0)\n",
    "        centers.append(center)\n",
    "    expanded_clusters = clusters\n",
    "    for pt in border:\n",
    "        distances = {}\n",
    "        for i, center in enumerate(centers):\n",
    "    #         print(\"point = \", pt, \" center = \", i)\n",
    "    #         print(scipy.spatial.distance.cosine(points[pt],center))\n",
    "            distances[i] = sp.spatial.distance.cosine(points[pt],center)\n",
    "    #     distances = \n",
    "    #     print(\"closest cluster for point %d = %d \" %(pt, min(distances, key=distances.get)))\n",
    "        closest_cluster = min(distances, key=distances.get)\n",
    "        expanded_clusters[closest_cluster].add(pt)\n",
    "    #     print(clusters[closest_cluster])\n",
    "    label , centroids, expanded_clusters\n",
    "    centroid_labels = [len(clusters)+1]* len(centroids)\n",
    "    for index, clstr in enumerate(expanded_clusters):\n",
    "        for n in clstr:\n",
    "            centroid_labels[n]= index\n",
    "    print(np.unique(centroid_labels))\n",
    "    final_labels = [0]*len(label)\n",
    "    for i,l in enumerate(label):\n",
    "        final_labels[i] = centroid_labels[l]\n",
    "    np.unique(final_labels)\n",
    "    return final_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# clusters: 144\n",
      "clusters:  [{0, 12}, {1, 41}, {2, 69, 39, 137, 52, 20, 182, 87, 185, 30}, {3}, {4}, {113, 51, 5, 31}, {6}, {7}, {98, 67, 8, 11, 142, 14, 145, 116, 59}, {104, 9, 174, 17, 181, 117}, {101, 74, 10, 108, 170, 19, 123, 62}, {13}, {15}, {16}, {25, 18, 188, 114}, {21}, {105, 141, 22}, {23}, {24, 121}, {26}, {146, 27}, {28}, {29}, {32, 144}, {33}, {34}, {35}, {36}, {37}, {38, 143}, {40}, {176, 42}, {43}, {44}, {45}, {46}, {47}, {48}, {49}, {50}, {53}, {54}, {55}, {56, 115}, {57}, {58}, {60}, {61}, {63}, {64}, {65}, {66}, {68}, {70}, {71}, {72}, {73}, {75}, {76}, {77}, {78}, {79}, {80}, {81}, {82, 151}, {83}, {84}, {130, 85}, {86}, {88}, {89}, {90}, {91, 139, 93}, {92}, {94}, {95}, {96}, {97}, {99}, {100, 191}, {102}, {103}, {106, 171}, {107}, {109}, {110}, {111}, {112}, {118}, {119}, {120, 178}, {122}, {124}, {125}, {126}, {127}, {128}, {129}, {194, 131}, {132}, {133}, {134}, {135}, {136}, {138}, {140}, {147}, {148}, {149}, {150}, {152}, {153}, {154}, {155}, {156, 183}, {157}, {158}, {159}, {160, 195}, {161}, {162}, {163}, {164}, {165}, {166}, {167}, {168}, {184, 169}, {172}, {173}, {175}, {177}, {179}, {180}, {186}, {187}, {189}, {190}, {192}, {193}, {196}, {197}, {198}, {199}]\n",
      "[  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
      "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
      "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
      " 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125\n",
      " 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143]\n"
     ]
    }
   ],
   "source": [
    "## The number of clusters vary depending upon the epss and minPts you pass\n",
    "eps =0.5\n",
    "minPts = 1\n",
    "d = MyDBSCAN(points, eps,minPts)\n",
    "with open(\"submission1.txt\", \"w\") as f:\n",
    "            for l in d:\n",
    "                f.write(\"%s\\n\"%l)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Activity_dbscan_Using_NetworkX.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
